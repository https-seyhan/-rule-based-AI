{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Text Analysis and Preprocessing\n",
    "In this section, we will outline some steps that are used to preprocess text data so that it can be used to train a predictive model.  We will use Tweets by Barack Obama and Donald Trump as our data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Exploratory analysis using word clouds\n",
    "We can identify candidate feature variables in text using a word cloud visualisation, which shows the most frequently-used words in the text.\n",
    "### 7.1.1 Install and import required modules\n",
    "To install WordCloud using Anaconda Navigator:\n",
    "- Go to Environments.\n",
    "- Click on Channels.\n",
    "- Click on Add….\n",
    "- Type ‘https://anaconda.org/conda-forge’ and press Enter.\n",
    "- Click on Update channels.\n",
    "- Search packages for ‘wordcloud’ and install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Create a word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Obama’s Tweets.\n",
    "obama = pd.read_csv('BarackObamaTweets.csv')\n",
    "obama.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the text from the obama DataFrame in a Series\n",
    "obama_tweets = obama.text\n",
    "print(obama_tweets)\n",
    "type(obama_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all the tweets into a string and convert to lower case\n",
    "obama_str = str(obama_tweets).lower()\n",
    "\n",
    "# Create a Word Cloud using WordCloud()\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 2000,\n",
    "                      background_color = 'black',\n",
    "                      stopwords = STOPWORDS).generate(obama_str)\n",
    "\n",
    "# Plot the cloud using pyplot\n",
    "fig = plt.figure(figsize = (40, 30),\n",
    "                 facecolor = 'k',\n",
    "                 edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1\n",
    "Create a word cloud of Trump’s Tweets (from the file ‘DonaldTrumpTweets.csv’).\n",
    "Make the following adjustments:\n",
    "1. Change the background colour to white.\n",
    "2. Set the minimum font size to 10.\n",
    "3. Set the maximum number of words to 50.  \n",
    "  \n",
    "Documentation for Word Cloud: https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Data preparation - Bag-of-words (BOW)\n",
    "### 7.2.1 Simple example of a BOW\n",
    "We cannot use text data (e.g., a Tweet) in the mathematical models we use for prediction. So, we need a way to convert the text data into a numerical representation.  \n",
    "  \n",
    "Bag-of-words (BoW) is a simple method for doing this. It examines each observation (e.g., a single Tweet), treating each word as a feature variable, and the numeric value of the feature is the number of times the word occurs.  \n",
    "  \n",
    "For example, recall the two Donald Trump Tweets:  \n",
    "  \n",
    "  \n",
    "![Tweet 1](tweet1.png)\n",
    "![Tweet 2](tweet2.png)\n",
    "\n",
    "Using bag-of-words, these could be represented numerically as shown below.\n",
    "\n",
    "![bag_of_words](bag_of_words.png)\n",
    "\n",
    "Note:\n",
    "- The punctuation has been removed and all letters have been changed to lowercase.\n",
    "- The word “is” occurs thrice in the second Tweet, so its count is 3. It does not occur in the first Tweet, so its count is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2\n",
    "In an earlier exercise, we created the top row of the above table. It is the set of words that occur in both Tweets (called \"all_words\" in the code below). The program for creating this set is in the cell below.\n",
    "\n",
    "Now, use an appropriate method to generate a Bag of Words representation for the two Tweets, stored as a Pandas DataFrame. I.e., reproduce the above table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = \"We are building the wall...\"\n",
    "tweet2 = \"The Wall is being rapidly built! The Economy is GREAT! Our Country is Respected again!\"\n",
    "\n",
    "# Import the punctuation chars\n",
    "from string import punctuation\n",
    "\n",
    "# function to create a list of lowercase, non-punctuated words\n",
    "def clean_tweet(tweet):\n",
    "    lower_str = tweet.lower()\n",
    "    clean_str = \"\".join([ ch for ch in lower_str if ch not in punctuation ])\n",
    "    split_list = clean_str.split(' ')\n",
    "    return split_list\n",
    "\n",
    "tweet1_words = clean_tweet(tweet1)\n",
    "tweet2_words = clean_tweet(tweet2)\n",
    "all_words = set(tweet1_words + tweet2_words)\n",
    "\n",
    "print(tweet1_words)\n",
    "print(tweet2_words)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Bag-of-words representation of all the Obama and Trump Tweets \n",
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files into DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "obama = pd.read_csv('BarackObamaTweets.csv')\n",
    "trump = pd.read_csv('DonaldTrumpTweets.csv')\n",
    "print(len(obama))\n",
    "print(len(trump))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the data sets into one\n",
    "We need to combine the datasets, however, before doing so, we should add the target variable (i.e., what we want to be able to predict). In our case, we want to be able to predict whether a Tweet was written by Barack Obama or by Donald Trump. Remember, we cannot use text variables in a mathematical model, so we will create a new integer variable 'trump' and set it to 1 for all of Donald Trump's Tweets and 0 for all of Barack Obama's Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create target variable 'trump' indicating for each record whether it was written by Trump (1) or Obama (0)\n",
    "trump['trump'] = 1\n",
    "obama['trump'] = 0\n",
    "\n",
    "# Contatenate the two DataFrames into one\n",
    "alltweets = pd.concat([trump[['text', 'trump']], obama[['text', 'trump']]], axis = 0)\n",
    "print(len(alltweets))\n",
    "print(alltweets.head(5))\n",
    "print(alltweets.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a module to perform bag-of-words\n",
    "We are going to create a bag-of-words using a class called CountVectorizer, imported from the module 'sklearn.feature_extraction.text'.  \n",
    "  \n",
    "This class splits each Tweet into words (i.e., tokenises) and creates a vocabulary (i.e., a set) of all unique words in the corpus (i.e., body of work). It then creates a bag-of-words to count the frequency with which each vocabulary word occurs in each Tweet. The return value is a matrix that contains the unique words and the count for each word.  \n",
    "  \n",
    "For more information, see:  \n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Tweet text into a Pandas Series\n",
    "corpus = alltweets['text']\n",
    "print(type(corpus))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CountVectorizer class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectoriser object\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Perform two methods (fit and transform) from the CountVectorizer class\n",
    "# fit - Learn the vocabulary of all tokens in the raw text.\n",
    "# transform - Transform documents to 'document-term matrix' (i.e., bag-of-words).\n",
    "X = vect.fit_transform(corpus)\n",
    "\n",
    "# Inspect the dictionary contents (each unique word is a feature):\n",
    "print('Result is {} tokens = {}'.format(len(vect.get_feature_names()), vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search the corpus for unwanted words\n",
    "There are many strange words in the corpus, such as '0rtsmxdnfc'.  This is because the tweets include many website and picture links.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some of the items that may be unnecessary and worth removing\n",
    "def check_string(corpus, string):\n",
    "    found = [ tweet for tweet in corpus if string in tweet ]\n",
    "    print('String \"{}\" total={} found={}'.format(string, len(corpus), len(found)))\n",
    "    for f in found:\n",
    "        print(f)\n",
    "        print('---')\n",
    "    \n",
    "check_string(corpus, 'http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check the Twitter picture links\n",
    "check_string(corpus, 'pic.twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unwanted words using regular expressions\n",
    "A useful way to find and remove these unnecessary strings is to use \"regular expressions\".\n",
    "\n",
    "An example of a regular expression is ‘http\\S+’.  You can test this 'regex' using https://www.regexpal.com.  See what it finds when provided with the Tweet below:  \n",
    "  \n",
    "‘Tell your friends you fist bumped President Obama enter now for your chance. http://ofa.bo/fAEqÂ pic.twitter.com/zJEelmeIe6’. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use regular expressions to match and remove the website links. These start with ‘http’.  \n",
    "  \n",
    "A suitable regular expression is ‘http\\S+’, which means find the string ‘http’ followed by any sequence of non-whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all website links from the corpus\n",
    "\n",
    "# Module for regular expressions \n",
    "import re\n",
    "\n",
    "def clean_corpus(corpus, regex):\n",
    "    \n",
    "    # Use the Pandas Series 'apply' method to apply the lambda function to each element\n",
    "    corpus_cleaned = corpus.apply(\n",
    "        # Use the sub() function to replace each matched string with an empty string ('')\n",
    "        lambda x: re.sub(regex, '', x, flags = re.IGNORECASE)\n",
    "    )\n",
    "\n",
    "    # Create the BOW\n",
    "    vect = CountVectorizer()\n",
    "    X = vect.fit_transform(corpus_cleaned)\n",
    "    return (vect, X, corpus_cleaned)\n",
    "\n",
    "(tokens, X, corpus) = clean_corpus(corpus, r'http\\S+')\n",
    "print('Result is {} tokens remaining: {}'.format(len(tokens.get_feature_names()), tokens.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also remove the Twitter picture links that start with 'pic.twitter.com'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also remove words containing picture links:\n",
    "(tokens, X, corpus) = clean_corpus(corpus, r'pic.twitter.com\\S+')\n",
    "print('Result is {} tokens remaining: {}'.format(len(tokens.get_feature_names()), tokens.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also remove any words with digits or underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\S*\\d\\S*   remove digits (\\d) and all joined non-whitespace characters (\\S*)\n",
    "# \\w*_+\\w*   remove underscores (_+ where + is one or more) joined by word-space characters (\\w*)\n",
    "(tokens, X, corpus) = clean_corpus(corpus, r'\\S*\\d\\S*|\\w*_+\\w*')\n",
    "print('Result is {} tokens remaining: {}'.format(len(tokens.get_feature_names()), tokens.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the cleaned corpus\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "Note common words like \"is\", \"the\" and \"in\" in the cleaned corpus. Such words are known as 'stop words'; they are generally presumed to contribute little predictive power to a model and can therefore be removed.\n",
    "  \n",
    "The module used to extract our BOW includes a standard set of stop words that can be used as shown below. However, some care may be required in some applications to ensure that these stop words do not remove useful information.  \n",
    "  \n",
    "For more information, see:\n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#using-stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the cleaned corpus, removing stop words this time:\n",
    "vect = CountVectorizer(stop_words = 'english')\n",
    "bow = vect.fit_transform(corpus)\n",
    "\n",
    "print('Result is {} tokens remaining: {}'.format(len(vect.get_feature_names()), vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 BOW limitations\n",
    "A bag-of-words is a 'unigram' (where the unit of analysis n = 1) that separates the data into single words.  These unigrams cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn’t account for potential misspellings or word derivations.  \n",
    "  \n",
    "A more sophisticated approach is to use bigrams (where n = 2), where occurrences of pairs of consecutive words are counted.  The occurance of groups of characters is yet another approach, with these collections of character n-grams offering some resilience against misspellings and derivations.  \n",
    "\n",
    "See more here:\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#limitations-of-the-bag-of-words-representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Saving your pre-processed data\n",
    "Save your BOW and your combined data set. You will need them for the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the tweets\n",
    "alltweets.to_csv('alltweets.csv')\n",
    "\n",
    "# Store the cleaned corpus\n",
    "corpus.to_csv('corpus.csv', header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
